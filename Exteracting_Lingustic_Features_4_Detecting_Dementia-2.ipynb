{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exteracting-Lingustic-Features-4-Detecting-Dementia.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SZanM0SzDXR"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5qSL5jIklJPl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linguistic Markers for Dementia\n",
        "Automatic language assessment methods help clinicians detect linguistic markers associated with dementia. Here, we describe different linguistic markers such as incoherent speech, tangentiality and grammatical error, lexical retrieval difficulties, auditory comprehension difficulties, grammatical and spelling failures and present python scripts to extercat them."
      ],
      "metadata": {
        "id": "fumR4gRozETy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Incoherent speech "
      ],
      "metadata": {
        "id": "5gFhqQ-jzEpY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "A text with incoherent speech has not a relevant order between sentences. To measure rambling speech in a text, we can define a score based on the similarity score between sentences of the text. Thus, we first review different methods to measure text similritoes and various techniques to compute pairwise similarities between two sentences."
      ],
      "metadata": {
        "id": "cYVPYbCW54ix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Similrit Measures\n",
        "\n",
        "A good review of text similarit measuure has been provided in [1]. \n",
        "\n",
        "1.  Jaccard Similarity\n",
        "\n",
        "2.  Cosine Similarity\n",
        "\n",
        "2.1. Cosine Similarity using Spacyx\n",
        "2.2. Cosine Similarity using Scipy\n",
        "\n"
      ],
      "metadata": {
        "id": "3mCOXY6EATEu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Jaccard Similarity\n"
      ],
      "metadata": {
        "id": "wEZzH0DNz1CV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Jaccard_Similarity(text_1, text_2): \n",
        "    \n",
        "    # List the unique words in a document\n",
        "    words_list1 = set(text_1.lower().split()) \n",
        "    words_list2 = set(text_2.lower().split())\n",
        "    \n",
        "    # Find the intersection of words list of doc1 & doc2\n",
        "    intersection = words_list1.intersection(words_list2)\n",
        "\n",
        "    # Find the union of words list of doc1 & doc2\n",
        "    union = words_list1.union(words_list2)\n",
        "        \n",
        "    # Calculate Jaccard similarity score \n",
        "    # using length of intersection set divided by length of union set\n",
        "    return float(len(intersection)) / len(union)\n",
        "text_1 = \"Two children, a girl  reaching for the cookie jar.\"\n",
        "text_2 = \"Mother is She has a dish in your hand or is. \"\n",
        "\n",
        "Jaccard_Similarity(text_1,text_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BqKLZfkQ3Gz",
        "outputId": "79000380-e906-432a-9f98-835905bdb263"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.05263157894736842"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cosine Similarity\n",
        "\n"
      ],
      "metadata": {
        "id": "MfQ-5wpCS3n_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text_1 = \"Two children, a girl  reaching for the cookie jar.\"\n",
        "text_2 = \"Mother is She has a dish in your hand or is. \"\n",
        "words_list_1 = set(text_1.lower().split()) \n",
        "words_list_2 = set(text_2.lower().split()) \n",
        "# tokenization\n",
        "#words_list_1 = word_tokenize(text_1) \n",
        "#words_list_2 = word_tokenize(text_2)\n",
        "  \n",
        "# sw contains the list of stopwords\n",
        "#sw = stopwords.words('english') \n",
        "l1 =[];l2 =[]\n",
        "  \n",
        "# remove stop words from the string\n",
        "words_set_1 = {w for w in words_list_1} \n",
        "words_set_2 = {w for w in words_list_2}\n",
        "  \n",
        "# form a set containing keywords of both strings \n",
        "rvector = words_set_1.union(words_set_2) \n",
        "for w in rvector:\n",
        "    if w in words_set_1: l1.append(1) # create a vector\n",
        "    else: l1.append(0)\n",
        "    if w in words_set_2: l2.append(1)\n",
        "    else: l2.append(0)\n",
        "c = 0\n",
        "  \n",
        "# cosine formula \n",
        "for i in range(len(rvector)):\n",
        "        c+= l1[i]*l2[i]\n",
        "cosine = c / float((sum(l1)*sum(l2))**0.5)\n",
        "print(\"similarity: \", cosine)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apJY5ppSYQ8m",
        "outputId": "ea1048fc-0452-4837-fd2f-15408c392873"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "similarity:  0.10050378152592121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc1 = nlp(u'Two children, a girl  reaching for the cookie jar.') \n",
        "doc2 = nlp(u'Mother is She has a dish in your hand or is.')\n",
        "print (doc1.similarity(doc2)) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vfkrfKm-T4E",
        "outputId": "b86c2119-f8b8-41aa-aaad-f88241e5c250"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4966613486251336\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "!pip install tensorflow_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQQgXqlM-u4-",
        "outputId": "97c1fe75-afdd-433f-ee0a-68d6b32c57a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.7.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.13.3)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.42.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.10.0.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.22.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (12.0.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (57.4.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.10.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.0.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: tensorflow_hub in /usr/local/lib/python3.7/dist-packages (0.12.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_hub) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_hub) (3.17.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorflow_hub) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def heatmap(x_labels, y_labels, values):\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(values)\n",
        "\n",
        "    # We want to show all ticks...\n",
        "    ax.set_xticks(np.arange(len(x_labels)))\n",
        "    ax.set_yticks(np.arange(len(y_labels)))\n",
        "    # ... and label them with the respective list entries\n",
        "    ax.set_xticklabels(x_labels)\n",
        "    ax.set_yticklabels(y_labels)\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", fontsize=10,\n",
        "         rotation_mode=\"anchor\")\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    for i in range(len(y_labels)):\n",
        "        for j in range(len(x_labels)):\n",
        "            text = ax.text(j, i, \"%.2f\"%values[i, j],\n",
        "                           ha=\"center\", va=\"center\", color=\"w\", \n",
        "fontsize=6)\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "2I4_9lec_ywu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/1?tf-hub-format=compressed\"\n",
        "# Import the Universal Sentence Encoder's TF Hub module\n",
        "embed = hub.Module(module_url)\n",
        "\n",
        "# sample text\n",
        "messages = [\n",
        "# Smartphones\n",
        "\"My phone is not good.\",\n",
        "\"Your cellphone looks great.\",\n",
        "\n",
        "# Weather\n",
        "\"Will it snow tomorrow?\",\n",
        "\"Recently a lot of hurricanes have hit the US\",\n",
        "\n",
        "# Food and health\n",
        "\"An apple a day, keeps the doctors away\",\n",
        "\"Eating strawberries is healthy\",\n",
        "]\n",
        "\n",
        "similarity_input_placeholder = tf.placeholder(tf.string, shape=(None))\n",
        "similarity_message_encodings = embed(similarity_input_placeholder)\n",
        "with tf.Session() as session:\n",
        "    session.run(tf.global_variables_initializer())\n",
        "    session.run(tf.tables_initializer())\n",
        "    message_embeddings_ = session.run(similarity_message_encodings, feed_dict={similarity_input_placeholder: messages})\n",
        "\n",
        "    corr = np.inner(message_embeddings_, message_embeddings_)\n",
        "    print(corr)\n",
        "    heatmap(messages, messages, corr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "LcV-1tsk--Iq",
        "outputId": "ab47f533-f9e7-4fd8-c72a-c8242398e8d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-856722d9095d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodule_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://tfhub.dev/google/universal-sentence-encoder/1?tf-hub-format=compressed\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Import the Universal Sentence Encoder's TF Hub module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0membed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# sample text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_hub/module.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, spec, trainable, name, tags)\u001b[0m\n\u001b[1;32m    163\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No such graph variant: tags=%r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m     \u001b[0mabs_state_scope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_try_get_state_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmark_name_scope_used\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs_state_scope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_hub/module.py\u001b[0m in \u001b[0;36m_try_get_state_scope\u001b[0;34m(name, mark_name_scope_used)\u001b[0m\n\u001b[1;32m    401\u001b[0m       raise RuntimeError(\n\u001b[1;32m    402\u001b[0m           \u001b[0;34m\"variable_scope %s was unused but the corresponding \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m           \"name_scope was already taken.\" % abs_state_scope)\n\u001b[0m\u001b[1;32m    404\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mabs_state_scope\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: variable_scope module_1/ was unused but the corresponding name_scope was already taken."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def heatmap(x_labels, y_labels, values):\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(values)\n",
        "\n",
        "    # We want to show all ticks...\n",
        "    ax.set_xticks(np.arange(len(x_labels)))\n",
        "    ax.set_yticks(np.arange(len(y_labels)))\n",
        "    # ... and label them with the respective list entries\n",
        "    ax.set_xticklabels(x_labels)\n",
        "    ax.set_yticklabels(y_labels)\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", fontsize=10,\n",
        "         rotation_mode=\"anchor\")\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    for i in range(len(y_labels)):\n",
        "        for j in range(len(x_labels)):\n",
        "            text = ax.text(j, i, \"%.2f\"%values[i, j],\n",
        "                           ha=\"center\", va=\"center\", color=\"w\", \n",
        "fontsize=6)\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "KkBn88DL_mhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Embedding\n",
        "Word Embedding, a technique in the language modeling domain, is utilized to map words or phrases that can be mapped to vectors of real numbers with several dimensions using word embedding. \n",
        "Different methods have been proposed as word embeddings. Here we review practices that have been used in the language of patients with dementia. \n",
        "\n",
        "1. Bag-Of-Words\n",
        "\n",
        "2. Doc2vec \n",
        "\n",
        "3. Doc2vecC\n",
        "\n",
        "4. TF-IDF\n",
        "\n",
        "5. Word2Vec\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rJlytzbNlMJ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bag-Of-Words"
      ],
      "metadata": {
        "id": "ypqbCoQvbRRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        " \n",
        "vector = CountVectorizer()\n",
        "#\n",
        "# Create sample set of documents\n",
        "#\n",
        "docs = np.array([\n",
        "'Two children, a girl  reaching for the cookie jar. ',\n",
        "'Mother is She has a dish in your hand or is.',\n",
        "'They have a garden',\n",
        "'There is a window with curtains', \n",
        "'The window was open. ',\n",
        "'She is wearing a dress and apron. ',\n",
        "'Cookie jar coverage open']\n",
        ")\n",
        "#\n",
        "# Fit the bag-of-words model\n",
        "#\n",
        "bag = vector.fit_transform(docs)\n",
        "#\n",
        "# Get unique words / tokens found in all the documents. The unique words / tokens represents\n",
        "# the features\n",
        "#\n",
        "print(vector.get_feature_names())\n",
        "#\n",
        "# Associate the indices with each unique word\n",
        "#\n",
        "print(vector.vocabulary_)\n",
        "#\n",
        "# Print the numerical feature vector\n",
        "#\n",
        "print(bag.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zetHRqZ-bUSj",
        "outputId": "1d20ca9d-68a4-429c-c98d-d78ae0dd0850"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['and', 'apron', 'children', 'cookie', 'coverage', 'curtains', 'dish', 'dress', 'for', 'garden', 'girl', 'hand', 'has', 'have', 'in', 'is', 'jar', 'mother', 'open', 'or', 'reaching', 'she', 'the', 'there', 'they', 'two', 'was', 'wearing', 'window', 'with', 'your']\n",
            "{'two': 25, 'children': 2, 'girl': 10, 'reaching': 20, 'for': 8, 'the': 22, 'cookie': 3, 'jar': 16, 'mother': 17, 'is': 15, 'she': 21, 'has': 12, 'dish': 6, 'in': 14, 'your': 30, 'hand': 11, 'or': 19, 'they': 24, 'have': 13, 'garden': 9, 'there': 23, 'window': 28, 'with': 29, 'curtains': 5, 'was': 26, 'open': 18, 'wearing': 27, 'dress': 7, 'and': 0, 'apron': 1, 'coverage': 4}\n",
            "[[0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 2 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0]\n",
            " [1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0]\n",
            " [0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Doc2vec"
      ],
      "metadata": {
        "id": "SNHrUl3rnJi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "W6OpkMtzmhjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tangentiality"
      ],
      "metadata": {
        "id": "p9LO3BfzzE01"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grammatical error\n",
        "To find out the grammatical errors in a text that a patient has provided with Alzheimer's disease dementia, you can use TextBlob Library\n",
        "[TextBlob](https://pypi.org/project/textblob/0.9.0/), which is a text processing library. "
      ],
      "metadata": {
        "id": "t90-Lu2wzE_Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction to TextBlob\n"
      ],
      "metadata": {
        "id": "S1f9Iw0cRg5v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###*Installing TextBlob*"
      ],
      "metadata": {
        "id": "PMJcM2lSRtxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textblob==0.9.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyPLpH50RKt0",
        "outputId": "4046b1d7-cefb-47bf-8594-9c9a74dcccaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob==0.9.0 in /usr/local/lib/python3.7/dist-packages (0.9.0)\n",
            "Requirement already satisfied: nltk>=3.0 in /usr/local/lib/python3.7/dist-packages (from textblob==0.9.0) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.0->textblob==0.9.0) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YtCsk5fvzFJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "text=TextBlob('I havv a bad spelling')\n",
        "##For Spelling Correction, we can use correct() method to correct spell\n",
        "#textc=text.correct()\n",
        "#print(textc)\n",
        "print(text)\n",
        "print(text.correct())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJ7BPIioTKBZ",
        "outputId": "bda7355a-950a-4106-a6b2-dd0804be9bdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I havv a bad spelling\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to Gingerit\n",
        "Using gingerit,you can correcting spelling and grammar mistakes based on the context of complete entences."
      ],
      "metadata": {
        "id": "qSLPd4qc4JyF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Installing Gingerit*"
      ],
      "metadata": {
        "id": "A-o7G-TY4ncO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install gingerit\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tqwaBoKxeth",
        "outputId": "5930d53d-9559-4c12-ef1b-a4fa67a2c226"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gingerit\n",
            "  Downloading gingerit-0.8.2-py3-none-any.whl (3.3 kB)\n",
            "Collecting requests<3.0.0,>=2.25.1\n",
            "  Downloading requests-2.27.0-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 989 kB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.25.1->gingerit) (2.0.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.25.1->gingerit) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.25.1->gingerit) (2021.10.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.25.1->gingerit) (2.10)\n",
            "Installing collected packages: requests, gingerit\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed gingerit-0.8.2 requests-2.27.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gingerit.gingerit import GingerIt\n",
        "import pandas as pd\n",
        "text = \"Two chldren, a grl reching for the cooki jar\"\n",
        "parser = GingerIt()\n",
        "parser.parse(text)\n",
        "b=parser.parse(text)\n",
        "#pd.set_option('max_colwidth', 520)\n",
        "corrected_df = parser.parse(text)\n",
        "corrected_df = pd.DataFrame(corrected_df)\n",
        "corrected_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "L26B2Z-gxkF-",
        "outputId": "ba9df5d0-682f-40de-cbec-9b77079bb454"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-3b1fd449-1039-40ce-8e63-d751f23b158f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>result</th>\n",
              "      <th>corrections</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Two chldren, a grl reching for the cooki jar</td>\n",
              "      <td>Two children, a girl reaching for the cookie jar</td>\n",
              "      <td>{'start': 35, 'text': 'cooki', 'correct': 'cookie', 'definition': 'any of various small flat sweet cakes'}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Two chldren, a grl reching for the cooki jar</td>\n",
              "      <td>Two children, a girl reaching for the cookie jar</td>\n",
              "      <td>{'start': 19, 'text': 'reching', 'correct': 'reaching', 'definition': 'the act of physically reaching or thrusting out'}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Two chldren, a grl reching for the cooki jar</td>\n",
              "      <td>Two children, a girl reaching for the cookie jar</td>\n",
              "      <td>{'start': 15, 'text': 'grl', 'correct': 'girl', 'definition': 'a young woman'}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Two chldren, a grl reching for the cooki jar</td>\n",
              "      <td>Two children, a girl reaching for the cookie jar</td>\n",
              "      <td>{'start': 4, 'text': 'chldren', 'correct': 'children', 'definition': 'a young person of either sex'}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3b1fd449-1039-40ce-8e63-d751f23b158f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3b1fd449-1039-40ce-8e63-d751f23b158f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3b1fd449-1039-40ce-8e63-d751f23b158f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                           text  ...                                                                                                               corrections\n",
              "0  Two chldren, a grl reching for the cooki jar  ...                {'start': 35, 'text': 'cooki', 'correct': 'cookie', 'definition': 'any of various small flat sweet cakes'}\n",
              "1  Two chldren, a grl reching for the cooki jar  ...  {'start': 19, 'text': 'reching', 'correct': 'reaching', 'definition': 'the act of physically reaching or thrusting out'}\n",
              "2  Two chldren, a grl reching for the cooki jar  ...                                            {'start': 15, 'text': 'grl', 'correct': 'girl', 'definition': 'a young woman'}\n",
              "3  Two chldren, a grl reching for the cooki jar  ...                      {'start': 4, 'text': 'chldren', 'correct': 'children', 'definition': 'a young person of either sex'}\n",
              "\n",
              "[4 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Refrences\n",
        "\n",
        "[1]. [Overview of Text Similarity Metrics in Python](https://towardsdatascience.com/overview-of-text-similarity-metrics-3397c4601f50)"
      ],
      "metadata": {
        "id": "VyZxQ3ejyNfY"
      }
    }
  ]
}